{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T07:55:28.444123Z",
     "start_time": "2019-01-11T07:55:28.441688Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import laser_hockey_env as lh\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T07:55:28.642643Z",
     "start_time": "2019-01-11T07:55:28.639000Z"
    }
   },
   "outputs": [],
   "source": [
    "#np.set_printoptions(suppress=True)\n",
    "np.set_printoptions(threshold = 1e5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T21:03:35.086427Z",
     "start_time": "2018-12-20T21:03:35.082123Z"
    }
   },
   "source": [
    "# Normal Game Play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T07:55:29.539930Z",
     "start_time": "2019-01-11T07:55:29.525220Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'laser_hockey_env' from '/Users/shane/Desktop/Uni/ReinforcementLearning/final/laser-hockey-env-master/laser_hockey_env.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(lh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T07:55:29.930674Z",
     "start_time": "2019-01-11T07:55:29.926764Z"
    }
   },
   "outputs": [],
   "source": [
    "env = lh.LaserHockeyEnv(mode=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "have a look at the initialization condition: alternating who starts and are random in puck position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T07:57:12.690181Z",
     "start_time": "2019-01-11T07:57:12.662104Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  6.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  4.29283237, -0.76891804,  0.        ,\n",
       "        0.        ])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "#obs_agent2 = env.obs_agent_two()\n",
    "#env.render()\n",
    "obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Recovery Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:    \n",
    "    saver = tf.train.import_meta_graph('DDPG_model.ckpt.meta')\n",
    "    saver.restore(sess, 'DDPG_model.ckpt')\n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('./'))\n",
    "    graph = tf.get_default_graph()\n",
    "    \n",
    "    for op in graph.get_operations():\n",
    "        print(op.name)\n",
    "    \n",
    "    #Qs = tf.get_collection('pred_Qs')[0]\n",
    "    #Qs = graph.get_operation_by_name('DQNAgent_2/Q_target/pred').outputs[0]\n",
    "    #Qs = graph.get_tensor_by_name('DQNAgent/Q_target/dense_1/kernel:0')\n",
    "    \n",
    "    #state = tf.get_collection('state')\n",
    "    #state = graph.get_tensor_by_name('DQNAgent_2/Q_target/state:0')\n",
    "    #qs = np.argmax(sess.run(Qs, {state: np.asarray(obs).reshape(-1,16)}))\n",
    "    #print ('decision: ', qs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For DQN Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate using DQNAgent\n",
    "with tf.Session() as sess:    \n",
    "    saver = tf.train.import_meta_graph('DQN_model.ckpt.meta')\n",
    "    saver.restore(sess, 'DQN_model.ckpt')\n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('./'))\n",
    "    graph = tf.get_default_graph()\n",
    "    \n",
    "    #for op in graph.get_operations():\n",
    "    #    print(op.name)â€º\n",
    "    \n",
    "    #Qs = tf.get_collection('pred_Qs')[0]\n",
    "    Qs = graph.get_operation_by_name('DQNAgent_2/Q_target/pred').outputs[0]\n",
    "    #Qs = graph.get_tensor_by_name('DQNAgent/Q_target/dense_1/kernel:0')\n",
    "    \n",
    "    #state = tf.get_collection('state')\n",
    "    state = graph.get_tensor_by_name('DQNAgent_2/Q_target/state:0')\n",
    "    #state = graph.get_operation_by_name('DQNAgent/Q_target/state').outputs[0]\n",
    "    \n",
    "    #obs = np.asarray(obs).reshape(-1, 16)\n",
    "    #print ('obs: ', obs)\n",
    "    #qs = np.argmax(sess.run(Qs, {state: np.asarray(obs).reshape(-1,16)}))\n",
    "    \n",
    "    obs = env.reset()\n",
    "    for _ in range(1000):\n",
    "        env.render()\n",
    "        qs = np.argmax(sess.run(Qs, {state: np.asarray(obs).reshape(-1,16)}))\n",
    "        a1 = env.discrete_to_continous_action(qs)\n",
    "        a2 = np.random.uniform(-1,1,3)    \n",
    "        obs, r, d, info = env.step(np.hstack([a1,a2]))    \n",
    "        #print debug info to improve the reward definition\n",
    "        print (info)\n",
    "        obs_agent2 = env.obs_agent_two()\n",
    "        if d: break\n",
    "\n",
    "    print ('points: ', r)\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For DDPG Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from DDPG_model.ckpt\n",
      "Human Controls:\n",
      " left:\t\t\tleft arrow key left\n",
      " right:\t\t\tarrow key right\n",
      " up:\t\t\tarrow key up\n",
      " down:\t\t\tarrow key down\n",
      " tilt clockwise:\tw\n",
      " tilt anti-clockwise:\ts\n",
      "Player 2 scored\n",
      "Player 2 scored\n",
      "Player 2 scored\n",
      "Player 2 scored\n",
      "Player 1 scored\n",
      "Player 2 scored\n",
      "Player 2 scored\n",
      "Player 2 scored\n",
      "Player 2 scored\n",
      "Player 1 scored\n",
      "Player 1 scored\n",
      "Player 2 scored\n",
      "Player 2 scored\n",
      "Player 1 scored\n",
      "Player 2 scored\n",
      "Player 2 scored\n",
      "Player 2 scored\n",
      "Player 1 scored\n",
      "Player 2 scored\n",
      "Player 2 scored\n",
      "Player 2 scored\n",
      "Player 2 scored\n",
      "Player 2 scored\n",
      "Player 2 scored\n",
      "Player 2 scored\n",
      "Player 2 scored\n",
      "Player 2 scored\n",
      "Player 2 scored\n",
      "Player 2 scored\n",
      "Player 2 scored\n",
      "Player 1 scored\n",
      "Player 2 scored\n",
      "Player 2 scored\n",
      "Player 2 scored\n",
      "Player 2 scored\n",
      "Player 2 scored\n",
      "Player 2 scored\n",
      "Player 2 scored\n",
      "Player 2 scored\n",
      "Player 2 scored\n",
      "Player 2 scored\n",
      "Player 2 scored\n",
      "Player 2 scored\n",
      "Player 2 scored\n",
      "Player 2 scored\n",
      "Player 2 scored\n",
      "Player 2 scored\n",
      "Player 2 scored\n",
      "Player 2 scored\n",
      "Player 2 scored\n",
      "Player 2 scored\n",
      "Player 2 scored\n",
      "Player 2 scored\n",
      "Player 2 scored\n",
      "Player 2 scored\n",
      "Player 2 scored\n",
      "Player 2 scored\n",
      "Player 2 scored\n",
      "Player 1 scored\n",
      "Player 2 scored\n",
      "Player 2 scored\n",
      "Player 1 scored\n",
      "Player 2 scored\n",
      "Player 2 scored\n",
      "Player 2 scored\n",
      "Player 2 scored\n",
      "Player 2 scored\n",
      "Player 2 scored\n",
      "Player 2 scored\n",
      "Player 2 scored\n",
      "Player 2 scored\n",
      "Player 2 scored\n",
      "Player 2 scored\n",
      "Player 2 scored\n",
      "Player 2 scored\n",
      "Player 2 scored\n",
      "Player 2 scored\n",
      "winning rate:  0.07920792079207921 draw rate:  0.2376237623762376 losing rate:  0.6831683168316832\n"
     ]
    }
   ],
   "source": [
    "AGAINST_HUMAN = False\n",
    "\n",
    "with tf.Session() as sess:    \n",
    "    saver = tf.train.import_meta_graph('DDPG_model.ckpt.meta')\n",
    "    saver.restore(sess, 'DDPG_model.ckpt')\n",
    "    graph = tf.get_default_graph()\n",
    "    \n",
    "    playerHuman = lh.HumanOpponent(env=env, player=2)\n",
    "    playerComputer = lh.BasicOpponent()\n",
    "    \n",
    "    pred = graph.get_operation_by_name('DDPGAgent/Mu_target/predMu/Tanh').outputs[0]    \n",
    "    state = graph.get_tensor_by_name('DDPGAgent/Mu_target/state:0')\n",
    "    \n",
    "    ####### Test Output ######\n",
    "    #As = sess.run(pred, {state: np.asarray(obs).reshape(-1,16)})\n",
    "    #print (As[0])\n",
    "    win = 0\n",
    "    lose = 0\n",
    "    rounds = 101\n",
    "    for i in range(rounds):\n",
    "        obs = env.reset()\n",
    "        reward = 0\n",
    "        while(1):\n",
    "#             env.render()\n",
    "            As = sess.run(pred, {state: np.asarray(obs).reshape(-1,16)})[0]\n",
    "            if AGAINST_HUMAN:\n",
    "                a2 = playerHuman.act(env.obs_agent_two())\n",
    "            else:\n",
    "                a2 = playerComputer.act(env.obs_agent_two())    \n",
    "            obs, r, d, info = env.step(np.hstack([As,a2]))    \n",
    "            reward += r\n",
    "            \n",
    "            \n",
    "            #print ('d: ', info['reward_closeness_to_puck'], 'to line: ', info['defence_line_distance'], 'puck direction: ', info['reward_puck_direction'])\n",
    "#             print ('d: ', info['reward_closeness_to_puck'])\n",
    "#             if info['reward_touch_puck'] > 0 :\n",
    "#                 print ('reward after hit: ', reward)\n",
    "#             print (obs[3])\n",
    "            \n",
    "            if d:\n",
    "                if info['winner']==-1:\n",
    "                    lose += 1\n",
    "                    break;\n",
    "                elif info['winner']==1: \n",
    "                    win += 1\n",
    "                    break\n",
    "                else: break;\n",
    "\n",
    "        #print ('total points: ', reward)\n",
    "    env.close()\n",
    "    print ('winning rate: ', win/rounds, 'draw rate: ', (rounds-win-lose)/rounds, 'losing rate: ', lose/rounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T07:57:48.631793Z",
     "start_time": "2019-01-11T07:57:48.627528Z"
    }
   },
   "source": [
    "\"info\" dict contains useful proxy rewards and winning information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T08:00:20.784862Z",
     "start_time": "2019-01-11T08:00:20.779373Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'winner': 0,\n",
       " 'reward_closeness_to_puck': -0.016178733106732124,\n",
       " 'reward_touch_puck': 0.0,\n",
       " 'reward_puck_direction': -5.416407436132432e-06}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T07:59:24.867441Z",
     "start_time": "2019-01-11T07:59:24.862324Z"
    }
   },
   "source": [
    "Winner == 0: draw\n",
    "\n",
    "Winner == 1: you (left player)\n",
    "\n",
    "Winner == -1: opponent wins (right player)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T20:37:41.013424Z",
     "start_time": "2018-12-20T20:37:41.009298Z"
    }
   },
   "source": [
    "# Hand-crafted Opponent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T08:01:33.837983Z",
     "start_time": "2019-01-11T08:01:33.831404Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'laser_hockey_env' from '/home/shane/Desktop/ReinforcementLearning/final/laser-hockey-env-master/laser_hockey_env.py'>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(lh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T08:01:45.035969Z",
     "start_time": "2019-01-11T08:01:45.032057Z"
    }
   },
   "outputs": [],
   "source": [
    "env = lh.LaserHockeyEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T08:01:45.572163Z",
     "start_time": "2019-01-11T08:01:45.504321Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o = env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T08:01:49.157281Z",
     "start_time": "2019-01-11T08:01:49.152424Z"
    }
   },
   "outputs": [],
   "source": [
    "player1 = lh.BasicOpponent()\n",
    "player2 = lh.BasicOpponent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T08:01:52.268233Z",
     "start_time": "2019-01-11T08:01:52.264406Z"
    }
   },
   "outputs": [],
   "source": [
    "obs_buffer = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T08:04:40.694729Z",
     "start_time": "2019-01-11T08:04:32.333471Z"
    }
   },
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "obs_agent2 = env.obs_agent_two()\n",
    "for _ in range(600):\n",
    "    env.render()\n",
    "    a1 = player1.act(obs)\n",
    "    a2 = player2.act(obs_agent2)\n",
    "    obs, r, d, info = env.step(np.hstack([a1,a2]))    \n",
    "    obs_buffer.append(obs)\n",
    "    obs_agent2 = env.obs_agent_two()\n",
    "    if d: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-28T12:27:58.824246Z",
     "start_time": "2018-12-28T12:27:58.813103Z"
    }
   },
   "outputs": [],
   "source": [
    "obs_buffer = np.asarray(obs_buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-28T12:28:17.864363Z",
     "start_time": "2018-12-28T12:28:17.847595Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.21772583,  0.14022074, -0.50388751,  0.06938814,  0.42967239,\n",
       "       -0.05056231,  5.99394432,  0.190191  , -0.04062059,  1.15586982,\n",
       "       -0.13772885, -0.0704963 , -0.85041827,  0.12573967, -2.2194066 ,\n",
       "       -0.30853598])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(obs_buffer,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-28T12:28:31.513942Z",
     "start_time": "2018-12-28T12:28:31.499813Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.09465005,  1.7363143 ,  2.48617842,  4.01910657,  5.39319311,\n",
       "        2.70587981,  1.44744218,  1.54582286,  0.24750636,  4.41002617,\n",
       "        5.06345003,  0.4849688 ,  2.66636978,  2.25679011, 13.08127676,\n",
       "        7.6629061 ])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(obs_buffer,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaling = [ 1.0,  1.0 , 3.14, 4.0, 4.0, 2.0,  \n",
    "            1.0,  1.0,  3.14, 4.0, 4.0, 2.0,  \n",
    "            2.0, 2.0, 10.0, 10.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human Opponent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T08:06:22.038375Z",
     "start_time": "2019-01-11T08:06:22.035338Z"
    }
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T08:05:10.184886Z",
     "start_time": "2019-01-11T08:05:10.180414Z"
    }
   },
   "outputs": [],
   "source": [
    "env = lh.LaserHockeyEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T08:05:17.339971Z",
     "start_time": "2019-01-11T08:05:17.276199Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o = env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T08:05:26.969723Z",
     "start_time": "2019-01-11T08:05:26.966375Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human Controls:\n",
      " left:\t\t\tleft arrow key left\n",
      " right:\t\t\tarrow key right\n",
      " up:\t\t\tarrow key up\n",
      " down:\t\t\tarrow key down\n",
      " tilt clockwise:\tw\n",
      " tilt anti-clockwise:\ts\n"
     ]
    }
   ],
   "source": [
    "player1 = lh.HumanOpponent(env=env, player=1)\n",
    "player2 = lh.BasicOpponent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T08:06:26.444905Z",
     "start_time": "2019-01-11T08:06:23.849965Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "time.sleep(1)\n",
    "obs_agent2 = env.obs_agent_two()\n",
    "for _ in range(600):\n",
    "    env.render()\n",
    "    a1 = player1.act(obs)\n",
    "    a2 = player2.act(obs_agent2)\n",
    "    obs, r, d, info = env.step(np.hstack([a1,a2]))    \n",
    "    obs_agent2 = env.obs_agent_two()\n",
    "    print (info)\n",
    "    if d: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
